{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Machine Learning With Python - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the <a href=\"https://savan77.github.io/blog/machine-learning-part1.html\"> previous post</a>, I explained what is machine learning, types of machine learning, linear regression, logistic regression, various issues that we need to consider such as overfitting and at last I explained what really learning is in machine learning. In <a href=\"https://savan77.github.io/blog/lab-machine-learning-part1.html\">lab session</a>, I explained how to implement algorithms and concepts that I explained in theory session using Python.\n",
    "\n",
    "In this session, I will explain some easy yet powerful machine learning algorithms such as <b> naive bayes, support vector machine and decision trees</b>. From now onwards, I will not make seperate part for theory and lab session. Instead, I will integrate theory with code in jupyter notebook. If you are unfamiliar with Jupyter notebooks, please go through <a href=\"http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Notebook%20Basics.html\"> Jupyter Notebook Basics Guide </a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b> Naive Bayes </b> is a supervised learning algorithm which is based on <a href=\"https://en.wikipedia.org/wiki/Bayes%27_theorem\"><b> bayes theorem </b></a>. Naive Bayes is a widely used classification algorithm. Here, word <b> naive</b> comes from the assumption of independence among features. That is, if we have a feature vector (input vector) (x<sub>1</sub>, x<sub>2</sub>,...,x<sub>n</sub>), x<sub>i</sub><sup>'</sup>s are conditionally independent given <i>y</i>. We can write bayes theorem as follows :<br><br>\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "P( y | x ) = \\frac{P(y)P(x | y)}{P(x)}\n",
    "\\end{align}\n",
    "\n",
    "where,<br><br>\n",
    "P(x) is the prior probability of a feature.<br>\n",
    "P(x | y) is the probability of a feature given target. It's also known as likelihood.<br>\n",
    "P(y) is the prior probability of a target or class in case of classification.<br>\n",
    "p(y | x) is the posterior probability of target given feature.<br>\n",
    "<br>\n",
    "when we have more than one feature then we can rewrite this equation as :\n",
    "\n",
    "\\begin{align}\n",
    "P( y | x_1,...,x_n) = \\frac{P(y)P(x_1,...,x_n | y)}{P(x_1,...,x_n)}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an example for spam classification, our input or feature vector will be a set of words and output will be spam or ham (1 or 0). In naive bayes, we calculate probability of each class(spam or ham) given feature vector and class with maximum probability becomes our output. Our task is to solve above equation for each class. Now, let us dig deeper into this equation and see how we can use this equation to find the probability of each class.<br>\n",
    "Using the naive bayes assumption we can write :<br><br>\n",
    "\\begin{align}\n",
    "P(x_i | y, x_1,..,x_{i-1},x{i+1},..,x_n) = P(x_i | y)\n",
    "\\end{align}\n",
    "\n",
    "We can rewrite bayes theorem as follows : \n",
    "<br><br>\n",
    "\\begin{align}\n",
    "P( y | x_1,...,x_n) = \\frac{P(y)\\prod_{i=1}^{n}P(x_i| y)}{P(x_1,...,x_n)}\n",
    "\\end{align}\n",
    "\n",
    "but we know that <b> P(x<sub>1</sub>, x<sub>2</sub>, .., x<sub>n</sub>)</b> is constant given the input. So we can say that\n",
    "<br><br>\n",
    "\n",
    "\\begin{align}\n",
    "P(y|x_1,...,x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i|y)\\end{align}\n",
    "<br>\n",
    "For classification rule (where we want to find the class with maximum probability), we can write equation as :\n",
    "<br><br>\n",
    "\\begin{align}\n",
    "\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i|y)\n",
    "\\end{align}\n",
    "<br>\n",
    "Now, we can use <a href=\"https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation\"> Maximum a Posteriori</a> estimation to estimate both P(y) and P(x<sub>i</sub>|y). Here, P(y) = samples with class y / total number of sample, in other words, frequency of class y in training data. <br>\n",
    "\n",
    "We can make several variants of naive bayes by using different distribution for P(x<sub>i</sub>|y). Widely used Naive Bayes variants are <a href=\"https://en.wikipedia.org/wiki/Normal_distribution\"> Gaussian Naive Bayes </a>, <a href=\"https://en.wikipedia.org/wiki/Multinomial_distribution\"> Multinomial Naive Bayes </a> and <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\"> Bernoulli Naive Bayes </a>.\n",
    "\n",
    "Now, we will implement Naive Bayes algorithm in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n",
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\savan77\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#we will use iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "#load the dataset\n",
    "data = load_iris()\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(data.data, data.target)\n",
    "\n",
    "#evalaute\n",
    "print(model.score(data.data, data.target))\n",
    "\n",
    "#predict\n",
    "print(model.predict([4.2, 3, 0.9, 2.1])) #0 = setosa,1 = versicolor, and 2 = virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b> Support Vector Machines</b> are supervised learning models which can be used for both classification and regression. SVMs are among the best supervised learning algorithms. It is effective in high dimensional space and it is memory efficient as well.\n",
    "\n",
    "Consider a binary classification problem, where the task is to assign a one of the two labels to given input. We plot each data item as a point in n-dimensional space as follows:\n",
    "\n",
    "![title](./images/svm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform classification by finding the hyperplane that differentiate the two classes very well. As you can see in above image, we can draw m number of hyperplanes. How do we find the best one? We can find the optimal hyperplane by maximizing the <b> margin </b>.\n",
    "![title](./images/svm2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define margin as a distance between the hyperplane and the nearest sample points to the hyperplane. This points are known as <b>support vector</b>. In above figure, support vectors are represented with filled color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
