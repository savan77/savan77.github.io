<!DOCTYPE html>
<html lang="en">
<head>
  <title> Practical Machine Learning With Python - Part 1 </title>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="HandheldFriendly" content="True">
  <meta name="keywords" content="machine learning, python, practical machine learning, linear regression, logistic regression, overfitting, linear regression in python, logistic regression in python, types of machine learning, how to start with machine learning, beginner guide machine learning, cross validation in python, regularization in python, learning in machine learning, l1 and l2 regularization, machine learning code">
  <meta name="description" content="Learn what is machine learning, types of machine learning and simple machine learnign algorithms such as linear regression, logistic regression and some concepts that we need to know such as overfitting, regularization and cross-validation with code in python. This is a practical guide to machine learning using python. ">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta property="og:title" content="Practical Machine Learning With Python - Part 1" />
<meta property="og:type" content="website" />
<meta name="author" content="Savan Visalpara">
<meta property="og:url" content="http://savan77.github.io/blog/machine-learning-part1.html" />
<meta property="og:image" content="./images/lin_reg1.png" />
<meta property="og:image" content="./images/lin_reg2.png" />
<meta property="og:image" content="./images/lin_reg3.png" />
<meta property="og:image" content="./images/lin_reg4.png" />
<meta property="og:image" content="./images/lin_reg5.png" />
<meta property="og:image" content="./images/lin_reg6.png" />
<meta property="og:image" content="./images/grad_desc1.png" />
<meta property="og:image" content="./images/grad_desc2.png" />
<meta property="og:image" content="./images/cost_fun.png" />
<meta property="og:image" content="./images/hypothesis.png" />
<meta property="og:image" content="./images/sigmoid.png" />
<meta property="og:description" 
  content="Practical Machine Learning With Python - Part 1. Learn What is Machine Learning with Code in Python. This blog post explains what is machine learning and various machine learning algorithms with code in python." />

  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="../css/bootstrap.min.css">
  <meta name="google-site-verification" content="EE3dIolJG90ghbBxG9IPVNpVXjkhOX7LWJ_yfYvDBLk" />
  <script src="../css/jquery.min.js"></script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script src="../css/bootstrap.min.js"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/zepto/1.1.6/zepto.min.js"></script>
   
  <style>
  #points{
      margin-left: 70px;
  }
  #takeaways{
    margin-left:70px;
  }
  #title{
    background-color: #3399FF;
    height:200px;
    width:100%;
  }
  #title h1{
    color:#FFFFE0;
    font-family: "Palatino Linotype", "Book Antiqua", Palatino, serif;
    font-weight: bold;
    word-spacing: 4px;
    letter-spacing:3px;
    margin-top:70px;
  }
  #main-body{
    width:700px;
    text-align: center;
  }
  #main-body h1{
    font-weight:bold;
    color:#3399FF;
    font-family: lucida sans;
    padding: 30px 0px 30px 0px;
  }
  #title a{
    text-decoration: none;
  }
  #main-body p{
    word-wrap:break-word;
    font-size :18px;
    font-family: lucida sans;
  }
  
  #img{
    margin-top:30px;
    margin-bottom:30px;
    height:100px;
    width:500px;
    margin-left: -10px;
  }
  #cap{
    color:gray;
    font-style: italic;
  }
  #s-text{
    color:black;
    font-size: 22px;
    font-family: papyrus;
    font-weight: bold;
  }
  #head{
    margin:30px 0px 30px 0px;
  }
  #footer{
    height:80px;
    background-color: #3399FF;
    width:100%;

  }
  #footer p{
    color:white;
    font-family: "Palatino Linotype", "Book Antiqua", Palatino, serif;
    font-weight: bold;
    word-spacing: 4px;
    letter-spacing:3px;
    margin-top:30px;
  }
  #footer a{
    text-decoration: none;
    color:white;
  }
  #desc-blog{
    color:black;
    font-size: 15px;
    font-family: lucida sans;
  }
  th, td{padding:8px 10px 8px 10px;}
   @media (max-width: 768px){
    #title h1{
      font-size:19px;
    }
    #s-text{
      font-size:14px;
    }
    #desc-blog{
      font-size:14px;
    }
    #main-body{
      width:330px;
    }
    ol li{
      font-family: "Palatino Linotype", "Book Antiqua", Palatino, serif;
      font-size:18px;

    }
    ol li span{
      margin-right:-370px;
    }
    #outlier{
      margin-right: -400px;
    }
    ul li h3{
      margin-right: -370px;
    }
    #mob_img{
      margin-left:-63px;
      width:430px;
      height:370px;
    }
    #mob_img2{
      margin-left:-25px;
      height:280px;
      width:330px;
    }
    #mob_img3{
      height:280px;
      width:380px;
      margin-right: 0px;
    }
    #mob_img4{
      height:280px;
      width:380px;
      margin-left: -20px; 
    }
    #mob_img5{
      height:260px;
      width:340px;
      margin-left: -25px; 
    }
    #mob_img6{
      margin-left:-35px;
      height:320px;
      width:420px;
    }
    #mob_img7{
      margin-left:-35px;
      height:300px;
      width:400px;
    }
    #outlier1{
      margin-left:-400px;
    }
    
  }
   ol li{
          font-family: "Palatino Linotype", "Book Antiqua", Palatino, serif;
          font-size: 18px;}
  </style>
</head>
<body>

<div class="container text-center" id="title">
  <a href="../blog.html"><h1> <span id="s-text">  </span> BLOG - SAVAN VISALPARA <span id="s-text">  </span></h1>
  </a><br />
  <span id="desc-blog"> [ Posts on Machine Learning, Computer Vision, and NLP ] </span><br>
  <span id="time"> Posted on 20 October, 2017</span>
</div>
<div class="container " id="main-body"><br>
    <h2> A Short Note on Gradients </h2><br><br>
    <p align="justify"> Most of us use frameworks like TensorFlow and PyTorch to build and train machine learning models. Such frameworks allow us to focus on important things (i.e the architecure of the model) and let it do the rest of the work. Calculating gradients of the parameters is an example of the redundant work they do for us. Although these frameworks calculate gradients for us, it is always a good idea to know about various properties of them, and how they behave in different circumstances.</p><br>

    <p align="justify"> Let's say you know how to calculate gradients manually and you are implementing a deep neural network from scratch. When we calculate the gradients manually, its always a good idea to know whether they are correct or not. We can perform gradient checking to see how much accurate the computed gradients are. We use following formula to numerically approximate the derivative at a particular point \(\theta\).
    </p>
      <img id="img" src="./images/gradientcheck.png" alt="gradient checking">

    <p align="justify"> Here, EPSILON is a small constant(i.e \(10^{-4}\)). We can compare this value with the value computed by the function which we use for gradient calculation. Next, we will discuss how extreme value of gradients causes various problems in neural networks.</p> 

<br>
    <h2> Dead Units in ReLU</h2>
    <p align="justify"> Currently, very popular choice for non-linearity is ReLU, which stands for rectified linear unit [\(F(x)\) = \(max(0,x)\)] In ReLU, neuron fires when the input is greater than 0. Otherwise, it will be zero. Sometimes, a large gradients flowing through the neuron (with ReLU) could cause weight update in such a way that the neuron will never activate again. That is, every gradients flowing through that neuron will be zero. Such neurons are called "Dead Units/Neurons". Important thing to note here is that once a neuron ends up in this state, it's unlikely to recover. The reason is, the function gradient at 0 is also 0. The most probable reason behind this problem is [very high learning rate]. So, try decreasing the learning rate first when you face this problem. Another possible way to address this issue is by setting a small positive gradient for negative inputs. This type of units are called "Leaky ReLU".</p>
<br>
    <h2>Saturated Sigmoid Units</h2>
    <p align="justify">  One common problem with sigmoid unit is vanishing and exploding gradients. Vanishing gradient problem occurs when even large changes in parameters of early layers do not make any difference in the output of the network. That is, the gradients of the network's output with respect to the parameters of early layers of a network becomes very small. This results in no learning in early layers. Similarly, exploding gradients is the case when these gradients are too large. One possible solution for exploding gradient problem is the gradient clipping[link]. In case of vanishing gradient problem, the reason can be weight initialization especially when you are using sigmoid or tanh as a non-linearity. An improper weight initialization may cause neurons saturate and ultimately stop the learning.</p>
<br>
    <p align="justify"> One of the top highlights on <a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b">Medium</a> is from the post of Andrej Karpathy, which is as follows: <span style="color:#787878 "><i> if you’re using sigmoids or tanh non-linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn’t cause them to be fully saturated.</i></span> </p>
<br>
    <p align="justify"> So, make sure you carefully initialize the weights of the network and use ReLU or similar non-linearities.</p><br>

    <p align="justify"> Another property of the gradient, when using sigmoid, is that it achieves a maximum at 0.25. Please note that we are referring to the local gradient of the sigmoid function which is [z(1-z)]. Here, we achieve maximum when z=0.5. As we know, local gradient will be multiplied by the gradient recieved from previous layer to calculate the final gradient at this neuron. In other words, every time a gradinet signal flows through a sigmoid unit its magnitude will be reduced by 0.25. </p><br>

    <p align="justify"> We saw various properties and effects of gradients on our learning system. Knowledge of such properties helps us better understand the internals of the learning system and it may help in debugging as well. </p>

   


</div>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-99746800-1', 'auto');
  ga('send', 'pageview');

</script>
<p style="margin-left: 570px"> Share this post on social media : </p>
<div class="a2a_kit a2a_kit_size_32 a2a_default_style" style="margin-left: 570px">
<a class="a2a_button_facebook"></a>
<a class="a2a_button_twitter"></a>
<a class="a2a_button_google_plus"></a>
<a class="a2a_button_linkedin"></a>
<a class="a2a_button_email"></a>
</div><br><br>
<script async src="https://static.addtoany.com/menu/page.js"></script>
<!-- AddToAny END -->
</body>
</html> 
  