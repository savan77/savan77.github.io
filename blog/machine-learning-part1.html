<!DOCTYPE html>
<html lang="en">
<head>
  <title> Machine Learning - Part 1 </title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="../css/bootstrap.min.css">
  <script src="../css/jquery.min.js"></script>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script src="../css/bootstrap.min.js"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/zepto/1.1.6/zepto.min.js"></script>
   
  <style>
  #takeaways{
    margin-left:70px;
  }
  #title{
    background-color: #3399FF;
    height:200px;
    width:100%;
  }
  #title h1{
    color:#FFFFE0;
    font-family: "Palatino Linotype", "Book Antiqua", Palatino, serif;
    font-weight: bold;
    word-spacing: 4px;
    letter-spacing:3px;
    margin-top:70px;
  }
  #main-body{
    width:700px;
    text-align: center;
  }
  #main-body h1{
    font-weight:bold;
    color:#3399FF;
    font-family: lucida sans;
    padding: 30px 0px 30px 0px;
  }
  #title a{
    text-decoration: none;
  }
  #main-body p{
    word-wrap:break-word;
    font-size :18px;
    font-family: lucida sans;
  }
  
  #img{
    margin-top:30px;
    margin-bottom:30px;
    height:250px;
    width:850px;
    margin-left: -80px;
  }
  #cap{
    color:gray;
    font-style: italic;
  }
  #s-text{
    color:black;
    font-size: 22px;
    font-family: papyrus;
    font-weight: bold;
  }
  #head{
    margin:30px 0px 30px 0px;
  }
  #footer{
    height:80px;
    background-color: #3399FF;
    width:100%;

  }
  #footer p{
    color:white;
    font-family: "Palatino Linotype", "Book Antiqua", Palatino, serif;
    font-weight: bold;
    word-spacing: 4px;
    letter-spacing:3px;
    margin-top:30px;
  }
  #footer a{
    text-decoration: none;
    color:white;
  }
  #desc-blog{
    color:black;
    font-size: 15px;
    font-family: lucida sans;
  }
  th, td{padding:8px 10px 8px 10px;}
   @media (max-width: 768px){
    #title h1{
      font-size:19px;
    }
    #s-text{
      font-size:14px;
    }
    #desc-blog{
      font-size:14px;
    }
    #main-body{
      width:330px;
    }
  }
   ol li{
          font-family: "Palatino Linotype", "Book Antiqua", Palatino, serif;
          font-size: 18px;}
  </style>
</head>
<body>

<div class="container text-center" id="title">
  <a href="../blog.html"><h1> <span id="s-text">  </span> BLOG - SAVAN VISALPARA <span id="s-text">  </span></h1>
  </a><br />
  <span id="desc-blog"> [ Posts on Artificial Intelligence, Machine Learning, Computer Vision, NLP and Programming ] </span><br>
  <span id="time"></span>
</div>
<div class="container " id="main-body">
  <h2> Index </h2>
  <ol>
    <li><a href="#intro"><span style="position: relative; left: -262px">Introduction </span></a></li>
    
    <li><a href="#types"> <span style="position: relative; left: -205px">Types of Machine Learning </span></a></li>
    <li><a href="#types"> <span style="position: relative; left: -242px">Linear Regression </span></a></li> 
  <li><a href="#learning"> <span style="position: relative; left: -95px">Learning = Representation + Evaluation + Optimization</span> </a></li></ol>
  <h2> Introduction </h2><br>
  <p id="intro" align="justify">
      Machine Learning is the subfield of Artificial Intelligence, which gives "computers the ability to learn without being explicitly programmed." It provides a set of algorithms that iteratively learn from the data. For example, we can build a machine learning model which can detect objects in an image by training our model on a large image dataset (i.e imagenet). In this series of posts, I will try to explain various algorithms of machine learning and tips and tricks on applied machine learning without getting into much maths and theory. 

  </p><br>

  <p id="types">
    <p align="justify">
      We can divide various Machine Learning algorithms in three types which are as follows:</p><br>
    <ul>
      <li>
        <h3 style="margin-left: -409px"> Supervised Learning </h3>
        <p align="justify"> Currently, most of the machine learning products use <b> Supervised Learning</b>. In this, we have a set of features or inputs (i.e image - X) and our model will predict a target or output variable (i.e caption of an image - y).<br> <p style="text-align: center"> y = f(X) </p><p align="justify"> In other words, our model learns a function that maps inputs to desired outputs. That's why it is also called as <b>function approximation</b>. Sometimes, features are refered as <b>independent variables</b> and targets as <b>dependent variable</b>. Supervised learnign problems can be further grouped into classification and regression problems. When the output variable is a category, such as "spam" or "ham" then it is known as <strong> classification </strong> problem. When the output variable is a real value, such as "price of the house" then it is known as <strong> regression</strong> problem. Examples : Linear Regression, Decision Tree, KNN etc.</p>
      </li><br>
      <li>
        <h3 style="margin-left: -380px"> Unsupervised Learning </h3>
        <p align="justify"> In Unsupervised Learning, we have input data (X) but no corresponding output variable (y). Goal of unsupervised learning is to model the distribution of the data in order to learn more about the data. Unsupervised Learning problems can be futher grouped into clustring and association problems. When we want to discover inherent groupings in the input data it is known as <b> clustering</b> problem. When we want to discover rules that describe portions of the input data it is known as <b> association </b> problem.</p>
      </li><br>
      <li>
        <h3 style="margin-left: -372px"> Reinforcement Learning </h3>
        <p align="justify"> In Reinforcement Learning, an agent acts in an environment so as to maximize its rewards. It's kind of trail and error method. Agent learns from past experience and tries to capture best possible knowledge to make accurate decision. For example, an agent which learns to play atari game.
        </p>
      </li></ul><br>
      <p align="justify"> Now a days, supervised learning has dominated the machine learning field. Almost any machine learning products you see use supervised learning. However, Unsupervised and Reinforcement Learning are active and hot area of research. In case of supervised learning, we have a labelled dataset to train our model. Here, labelled means each input raw is labelled with some output variable. For example, each image in a dataset might be labelled with the name of an object it contains. But, when we have an input data (X) but only few of them are labelled then this type of problem is known as <b> Semi-supervised Learning</b>. We will later discuss the concept called <b> Transfer Learning </b>, Andrew Ng said during his NIPS 2016 tutorial that transfer learning will be --after supervised learning-- the next driver of ML commercial success.</p>
      
  </p>
  <div id="lin_reg">
    
    <h2> Linear Regression </h2><br>
    <p align="justify">
      Now, we will see various supervised learning algorithms and we will start with very simple yet powerful one called <b> Linear Regression</b>. As I mentioned earlier, supervised learning problems can be divided into two types of problems- <i>Regression</i> and <i> Classification</i>. As name suggests, linear regression is used for regression problems, that is, when target variable is a real value.</p>
    <p align="justify"> Let's start with an example, suppose we have a dataset containing house area and the price of the house and our task is to build a machine learning model which can predict the price of the house given the area of the house. Here is how our dataset looks like,</p><br>
    <table border="1" border-collapse="collapse" align="center">
      <tr>
        <th> area (sq.ft)</th>
        <th> price (1k$s)</th>
      </tr>
      <tr>
        <td> 3456 </td>
        <td> 600 </td>
      </tr>
      <tr>
        <td> 2089 </td>
        <td> 395 </td>
      </tr>
      <tr>
        <td> 1416 </td>
        <td> 232 </td>
      </tr>
    </table><br>
  <p align="justify"> In linear regression, our task is to establish a relationship between target variable and input variables by fitting a line. This line can be represented as a linear equation <b> y = m * X + b</b>. Where, y - target variable, X - input data, m - slope, b - intercept. An intercept represents the intersection of regression line with y axis. Now we can plot our dataset as follows :</p>
   
  <img  src="./images/lin_reg1.png" alt="linear regression plotting"> 
        <figcaption>[ Figure - 1 ]</figcaption>

  <p align="justify">Apparently, we can fit <b>n</b> number of lines by tweaking coefficients <b>m</b> and <b>b</b> as shown below.</p>
  <img id="img" src="./images/lin_reg2.png" alt="line fitting" >
  <figcaption>[ Figure - 2 ]</figcaption>
  <p align="justify"> We can rewrite our equation as <b> y(x) = w<sub>0</sub> + w<sub>1</sub> * x </b>where, w<sub>0</sub> is the bias term and w<sub>1</sub> is the slope. Here, w<sub>i</sub>'s are known as the weights or parameters of our model. This equation can be used when we have one input variable (feature or independent variable) and this is called as <b> simple linear regression </b>. But this is not the ideal case. We usually deal with the datasets which have many features, the case when we have more than one features it is known as <b> multiple linear regression </b>. We can generalize our previous equation for simple linear regression to multiple linear regression -  \(y(x) = w_0 x_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n \). Here, x<sub>0</sub> = 1 and w<sub>0</sub> , as I mentioned earlier, is the y axis intercept. To simplify the notation, we rewrite the above equation as follows:<br><br>
  \(y(x)\) = \(\sum\limits_{i=0}^n w_i x_i = w^Tx\)
  </p>
<br>

  <p align="justify"> Here, x<sub>0</sub> = 1 since this is the intercept term. So, by changing the weights we can get different lines and now our task is to find weights for which we get best fit. One question you might have is, how can we determine how bad or good a particular line fits our data?. For this, we introduce a cost function which measures, for each value of m, how close the y's are to corresponding y<sub>true</sub>'s. We define our cost function as simple sum of sqaured error. Here, 1/2 is added to make derivation easy as we will see later.<br><br>

  \(J(w)\) = \(\frac{1}{2}\sum\limits_{i=1}^n (y(x^i) - y_t^i)^2\)  </p><br>
  <p align="justify"> As I mentioned earlier, this cost function calculates how far y's are from the corresponding y<sub>true</sub>'s. We can show this in the graph as shown in figure - 3. Basically, our cost function calculates the distance between true target and predicted target which is shown in the graph as lines between sample points and the regression line. This is called <b> residual</b>. </p>
  <img src="./images/lin_reg3.png" alt="cost function visualization">
  <figcaption>[ Figure - 3 ]</figcaption>
  <p align="justify"> For each value of weight, there will be some cost or an error. We want to find the value of weights for which cost is minimum. We can visualize this as follows : </p>
  <img src="./images/grad_desc1.png" alt="gradient descent">
  <figcaption>[ Figure - 4 ]</figcaption>
  <p align="justify"> As you can see, we get some value of cost/error for each value of weight. Here, I have used the word "global" and the reason is- In case of non-convex shape there will be more than one minimum but we want to find out the global minimum. For simplicity, I have used convex shape in above figure.</p>
  
  <h3> Gradient Descent </h3>
  <p align="justify">  Essentially, what we do in supervised learning problems is finding the best weights for our model. Hence, our task becomes optimization task. <b> Gradient Descent</b> is one of the most popular and widely used optimization algorithm. Gradient descent can be used to minimize a cost function \(J(w)\) parameterized by a model's parameters \(w \in \mathbb{R}^d \) by updating the parameters/weights in the opposite direction of the gradient of the cost function \(\nabla_w J(w)\) w.r.t to the parameters. Mathematically we can write as <br><br> \(w_n = w - \eta \cdot \nabla_w J( w)\) <br><br> Here, \(\eta\) is the learning rate which determines the size of the steps we take to reach a minimum. We need to be very careful about this parameter since high value of \(\eta\) may overshoot the minimum and very low value will reach minimum very slowly.</p></br>
  <img src="./images/grad_desc2.png">
  <figcaption>[ Figure - 5 ]</figcaption>
  <p align="justify"> There are three variants of gradient descent- Batch Gradient Descent, Stochastic Gradient Descent and Mini-batch Gradient Descent. <b> Batch gradient descent</b> computes the gradient of the cost function w.r.t to parameter w for <b> entire training data</b>. Since we need to calculate the gradients for the whole dataset to perform one parameter update, batch gradient descent can be very slow. <b> Stochastic gradient descent</b> computes the gradient for <b>each training example \(x^i\).</b> <b> Mini-batch gradient descent</b> computes the gradient for <b>every mini-batch of m training example</b>. If you are curious about gradient descent and its variants then check out <a href="http://sebastianruder.com/optimizing-gradient-descent/">this post</a> by Sebastin Ruder.

  <p align="justify"> In order to complete this algorithm we have to calculate the partial derivative term \(\nabla_wJ(w)\). Let us first derive equations for one training example and later we will modify it for more than one training example.<br><br>
    
    \(\nabla_wJ(w)\) = \(\frac{\partial}{\partial w_j} J(w)\) = \(\frac{\partial}{\partial w_j} \frac{1}{2} ( y(x) - y_t))^2\) <br>   
     &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  = \(2 \cdot\frac{1}{2} (y(x) -y_t) \cdot\frac{\partial}{\partial w_j} (y(x) - y_t) \)  <br>
     &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = \((y(x) - y_t) \cdot\frac{\partial}{\partial w_j} (\sum\limits_{i=0}^n w_i x_i - y_t)\) <br>
     &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = \((y(x) - y_t) x_j\)<br><br>
    Now, we can rewrite our update rule as follows : <br><br>
    \(w_n\) = \(w - \eta \cdot (y(x) - y_t) x_j\)   &nbsp; (Repeat until convergence)<br><br>

    We can modify this equation for more than one training example as follows : <br><br>
        \(w_n\) = \(w - \eta \cdot \sum\limits_{i=1}^n (y(x) - y_t) x_j\) <br><br>

    <p align="justify"> Note that we are updating our weights after going through entire dataset, that is, at each step we loop iterate through entire training dataset and then we update the weights. Can you recall the name of this variant of gradient descent? This is, <b> Batch Gradient Descent</b>. For stochastic gradient descent, we just need to remove the summation symbol from above equation. In practice, we use <b> mini-batch gradient descent</b> which takes best of both and performs update after going through a batch of n example. We can write update rule as:<br><br>

  \(w_n = w - \eta \cdot \nabla_w J( w ; x^{(i:i+n)} ; y^{(i:i+n)})\)<br><br>

    <p align="justify"> Seems like too much work for this simple linear regression, but believe me it's not, it's very easy. Let me briefly write some takeaways,<br><br>
    <div id="takeaways">
      <p align="justify"> 1. For simple linear regression, equation of line is <b> y(x) = w<sub>0</sub> + w<sub>1</sub> * x<sub>1</sub></b>. Here, w<sub>0</sub> is the intercept term and w<sub>1</sub> is the slope of the line. We want to find w<sub>0</sub> and w<sub>1</sub> such that it minimize our cost function [eq num]. Both of these variables are known as <b> weights</b> or <b> parameters</b> of our model. We can easily extend this equation for multiple linear regression as shown in equation [number].</p>
      <p align="justify"> 2. We start with <b>small positive random weights </b>.</p>
      <p align="justify"> 3. Calculate the loss/cost/objective function as follows : <br><br>
          \(J(w)\) = \(\frac{1}{2}\sum\limits_{i=1}^n (y(x^i) - y_t^i)^2\)<br>
      </p>
      <p align="justify"> 4. We update our weights as follows : <br><br>
             \(w_n\) = \(w - \eta \cdot \sum\limits_{i=1}^n (y(x) - y_t) x_j\)  (we derived this equation above)<br> </p>
      <p align="justify"> 5. Repeat steps <b> 3 </b> and <b> 4</b> until convergence. In other words, repeat step 3 and 4 until \(J(w)\) < 0.0003. The number 0.0003 was chosen randomly.</p>
    </div>















    <h2> Learning = Representation + Evaluation + Optimization </h2>
    <p align="justify"> You might have concluded in your mind that supervised learning is all about finding weights which minimize our cost. And that's true. But let us dig deeper and gain some insights on how learning really works. We will discuss this in depth for the classification problems since there are the widely used ones. We can divide learning phase in three parts - <b> Representation, Evaluation </b> and <b> Optimization </b>.</p>
    <p align="justify"> <b> Representation : </b> Choosing a representation of a learner is tantamount to choosing the set of classifiers that it can possibly learn. This set is called <b> hypothesis space</b>.</p>
    <p align="justify"> <b> Evaluation : </b> An evaluation or objective function is needed to distinguish good classifiers from bad ones.</p>
    <p align="justify"> <b> Optimization : </b> We need a method to find the best classifier in hypothesis space. Here comes the optimization. The choice of optimization technique determines the efficiency of a learner. Gradient descent optimization technique which we discussed earlier is the first-order optimization technique.</p>
  
    <p align="justify"> One of the most important problem in the machine learning is called <b> Overfitting</b>. Overfitting happens when a model performs too well on training data but does not perform well on unseen data. It happens when a model learns the noise present in the training data. Conversely, when a model does not perform well on training data as well as unseen data then that is known as <b> Underfitting</b>.</p>
    <img src="./images/lin_reg4.png">
    <figcaption>[ Figure - 6 ]</figcaption>
</div>

</body>
</html> 
  